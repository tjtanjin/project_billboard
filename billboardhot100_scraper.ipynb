{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILLBOARD HOT 100 SCRAPER\n",
    "\n",
    "## - Input & Output\n",
    "__Input__  = __Latest Date and Earliest Date__ as lists of [YYYY, MM, DD] <br>\n",
    "(note: just input single digit if day or month is single digit ; if scraping one chart, Latest Date = Earliest Date)\n",
    "\n",
    "__Output__ = __Dataframe of unique songs' details__, scraped from Billboard Hot 100 charts, for Latest Date to Earliest Date inclusive\n",
    "\n",
    "e.g. <br> \n",
    "Input = Latest Date [2018, 12, 29] and Earliest Date [2018, 1, 1] <br>\n",
    "Output = Dataframe of unique songs' details from 52 x Billboard Hot 100 charts, for 06 Jan 2018 to 29 Dec 2018 inclusive <br>\n",
    "\n",
    "## - How to Use\n",
    "1) Run all cells except last cell <br>\n",
    "2) In last cell: only input YYYY, MM and DD each for latest (i.e. Latest Date) and earliest (i.e. Earliest Date) <br>\n",
    "3) Run last cell\n",
    "\n",
    "## - Flaws (further development?)\n",
    "\n",
    "(Input) no error-catcher for wrong input of date e.g. Apr instead of 4 <br>\n",
    "(\"Back-from-the-Dead\") does not account for songs that had a Billboard streak, dropped out and then reappeared on chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rally library army\n",
    "from datetime import date, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from time import time, sleep\n",
    "from random import randint\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Billboard Hot 100 charts are Saturday weeklies, with Sundays to Fridays belonging to the same week as the UPCOMING Saturday\n",
    "# which_saturday() takes in a date as parameter, and returns which Saturday it belongs to (based on Billboard logic)\n",
    "# integrated into scrape_hot100()\n",
    "\n",
    "def which_saturday(date):\n",
    "    \n",
    "    if date.isoweekday() < 6:                               # .isoweekday() returns int of day of week, Monday = 1, Sunday = 7\n",
    "        days_to_saturday = 6 - date.isoweekday()            # if date is Monday to Friday, find number of days to UPCOMING Saturday   \n",
    "        date = date + timedelta(days = days_to_saturday)    # add (number of days to UPCOMING Saturday) to the date, it becomes UPCOMING Saturday\n",
    "    \n",
    "    if date.isoweekday() == 7:\n",
    "        date = date + timedelta(days = 6)                   # if date is Sunday, add 6 days to make it UPCOMING Saturday\n",
    "        \n",
    "    else:                                                   # if date is already Saturday, leave it\n",
    "        pass\n",
    "    \n",
    "    return date                                             # returns date as UPCOMING Saturday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_hot100() takes Latest Date and Earliest Date as parameters, and returns dataframe of unique songs' details\n",
    "\n",
    "def scrape_hot100(latest_date, earliest_date):\n",
    "    \n",
    "    # ----- convert Latest Date and Earliest Date to UPCOMING Saturdays (see cell above) ------ #\n",
    "    \n",
    "    latest_date = which_saturday(latest_date)           \n",
    "    earliest_date = which_saturday(earliest_date)\n",
    "\n",
    "    # ------ starting from Latest Date's chart, scrape weekly charts until and including Earliest Date's chart -------#\n",
    "    \n",
    "    bb_scrape = pd.DataFrame(columns=[\"name\", \"chart_scraped\", \"artist\", \"peak_position\", \"weeks_on_chart\"])   # prepare dataframe\n",
    "    scrape_date = latest_date           # start scraping from Latest Date\n",
    "    start_time = time()                 # set start_time to track scrape duration\n",
    "    \n",
    "    while scrape_date >= earliest_date:        # keep scraping from Latest Date and backwards, then stop after Earliest Date\n",
    "        \n",
    "        zero_singledigit = [\"blank to make index = value\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\",\n",
    "                           \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\",\n",
    "                           \"26\", \"27\", \"28\", \"29\", \"30\", \"31\"]  # prepares single digits with zeros to suit chart's url format\n",
    "        \n",
    "        # if date's month and/or day is a single digit, need to replace with double digits i.e. zero in front, then put into url\n",
    "        if (len(str(scrape_date.month)) == 1) or (len(str(scrape_date.day)) == 1):\n",
    "            \n",
    "            scrape_date_text = [scrape_date.year, zero_singledigit[scrape_date.month], zero_singledigit[scrape_date.day]]\n",
    "            url = \"https://www.billboard.com/charts/hot-100/{}-{}-{}\".format(scrape_date_text[0], \n",
    "                                                                             scrape_date_text[1], scrape_date_text[2])\n",
    "        \n",
    "        else:\n",
    "            url = \"https://www.billboard.com/charts/hot-100/{}-{}-{}\".format(scrape_date.year, scrape_date.month, scrape_date.day)\n",
    "        \n",
    "        # request Billboard Hot 100 chart, convert to Beautiful Soup, then for each chart item scrape its name, chart date, (...)\n",
    "        # (...) peak position and weeks on chart\n",
    "        r = requests.get(url)\n",
    "        r_soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        song_containers = r_soup.find_all(\"div\", attrs = {\"class\":\"chart-list-item\"})\n",
    "\n",
    "        for n in range(len(song_containers)):\n",
    "    \n",
    "            name = song_containers[n].find(\"span\", class_ = \"chart-list-item__title-text\").text.strip()\n",
    "            hex_name = name.replace(\" \", \"%20\")  # prepare Spotify API-friendly name\n",
    "            chart_scraped = scrape_date\n",
    "            artist = song_containers[n].find(\"div\", class_ = \"chart-list-item__artist\").text.strip()\n",
    "            try:       # for songs without Peak Position and Weeks on Chart, put NaN\n",
    "                peak_position = song_containers[n].find(\"div\", class_ = \"chart-list-item__weeks-at-one\").text.strip()\n",
    "                weeks_on_chart = song_containers[n].find(\"div\", class_ = \"chart-list-item__weeks-on-chart\").text.strip()\n",
    "            except:\n",
    "                peak_position = \"NaN\"\n",
    "                weeks_on_chart = \"NaN\"\n",
    "            \n",
    "            bb_scrape = bb_scrape.append({\"name\": hex_name, \"chart_scraped\": scrape_date, \"artist\": artist,\n",
    "                                         \"peak_position\": peak_position, \"weeks_on_chart\": weeks_on_chart}, \n",
    "                                         ignore_index = True)\n",
    "        \n",
    "            charts_total = int((latest_date - earliest_date).days / 7 + 1)       # counts total number of charts\n",
    "            charts_scrapped = int((latest_date - scrape_date).days / 7 + 1)      # tracks number of charts scraped\n",
    "            \n",
    "            # print progress\n",
    "            print(\"Busy scraping. Scraped/Total: {}/{}. Elapsed Time: {} mins\"\n",
    "                  .format(charts_scrapped, charts_total, round((time()-start_time)/60, 2)), end = \"\\r\", flush = True) \n",
    "        \n",
    "        sleep(randint(1,2))                                      # sleep between charts\n",
    "        scrape_date = scrape_date - timedelta(days=7)            # prepare to scrape the previous Saturday\n",
    "\n",
    "    # -------- sort scraped data by chart_scraped, i.e. latest chart's items on top, (...)\n",
    "    # (...) then remove duplicates based on song name, keeping only the latest data for that song name, (...)\n",
    "    # (...) since the latest data should be the most relevant e.g. peak position and weeks on chart --------- #\n",
    "    \n",
    "    bb_scrape = bb_scrape.sort_values(\"chart_scraped\", ascending = False)       \n",
    "    bb_scrape.drop_duplicates(\"name\", keep = \"first\", inplace = True)           \n",
    "    bb_scrape.reset_index(drop = True, inplace = True)           # resets index labels which were messed up during sorting\n",
    "    \n",
    "    return bb_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latest = [2018, 12, 29]        # INPUT: [YYYY, MM, DD] ; if month/day is single digit, just put single digit\n",
    "earliest = [2018, 1, 1]        # INPUT: [YYYY, MM, DD] ; if month/day is single digit, just put single digit\n",
    "\n",
    "# ---- after input, don't touch anything else, run this last cell ---- #\n",
    "scrape_hot100(date(latest[0], latest[1], latest[2]), date(earliest[0], earliest[1], earliest[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
